{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 6)\n"
     ]
    }
   ],
   "source": [
    "# The following code reads the input data set and creates a train and test data set from the all records\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# column names for dataframe\n",
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "def main():\n",
    "    # read training data\n",
    "    df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding = 'ISO-8859-1',names=cols)\n",
    "    # shuffling the records\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    # splitting the original dataset into train and test randomly in 99:1 ratio\n",
    "    np.random.seed(777)\n",
    "    msk = np.random.rand(len(df)) < 0.99\n",
    "    train = df[msk].reset_index(drop=True)\n",
    "    train= train[:300000]# using only 300000 rows for training\n",
    "    test = df[~msk].reset_index(drop=True)\n",
    "    # save both train and test as CSV files\n",
    "    train.to_csv('train_data.csv')\n",
    "    print(train.shape)\n",
    "    test.to_csv('test_data.csv')\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aritra/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:99: UserWarning: SparkContext already exists in this scope\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from /home/aritra/CS 631/Project/Birendra\n",
      "preprocessing data...\n",
      "training...\n",
      "making predictions on test data...\n",
      "test data accuracy 0.7934877350267713\n",
      "predictions saved to /home/aritra/CS 631/Project/Birendra/out\n",
      "saving model to /home/aritra/CS 631/Project/Birendra/model\n",
      "reading data from /home/aritra/CS 631/Project/Birendra\n",
      "preprocessing data...\n",
      "making predictions on test data...\n",
      "accuracy of the model is 0.7934877350267713\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import libraries\n",
    "import sys\n",
    "import findspark\n",
    "findspark.init(\"/home/aritra/spark/spark-2.4.0-bin-hadoop2.7\")\n",
    "#import pyspark as ps\n",
    "import warnings\n",
    "import re\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import Tokenizer, NGram, CountVectorizer, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import PipelineModel\n",
    "import os\n",
    "import findspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import Row, SQLContext\n",
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql.types import StringType\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# specify the parameters for saving and retreiving the model in disk \n",
    "inputdir = cwd\n",
    "outputfile = \"/home/aritra/CS 631/Project/Birendra/out\"\n",
    "modeldir = \"/home/aritra/CS 631/Project/Birendra/model\"\n",
    "\n",
    "# Regex pattern for pre-processing\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1,pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "# preprocessing codes\n",
    "# remove Twitter handle and URL, remove URL pattern starting with www., and transform to lower characters and remove numbers and special characters\n",
    "\n",
    "def pre_processing(column):\n",
    "    step1 = re.sub(combined_pat, '', column)\n",
    "    step2 = re.sub(www_pat, '', step1)\n",
    "    step3 = step2.lower()\n",
    "    step4 = neg_pattern.sub(lambda x: negations_dic[x.group()], step3)\n",
    "    final = re.sub(r'[^A-Za-z ]','',step4)\n",
    "    return final.strip()\n",
    "\n",
    "# building a pipeline following below order\n",
    "# tokenizer + create n-gram + count vceorizer + inverse doc freq + assembler+  encoding target labels\n",
    "\n",
    "def build_pipeline():\n",
    "    tokenizer = [Tokenizer(inputCol='tweet',outputCol='words')]\n",
    "    ngrams = [NGram(n=i, inputCol='words', outputCol='{0}_grams'.format(i)) for i in range(1,4)]\n",
    "    cv = [CountVectorizer(vocabSize=5460, inputCol='{0}_grams'.format(i), outputCol='{0}_tf'.format(i)) for i in range(1,4)]\n",
    "    idf = [IDF(inputCol='{0}_tf'.format(i), outputCol='{0}_tfidf'.format(i), minDocFreq=5) for i in range(1,4)]\n",
    "    assembler = [VectorAssembler(inputCols=['{0}_tfidf'.format(i) for i in range(1,4)], outputCol='features')]\n",
    "    label = [StringIndexer(inputCol='sentiment', outputCol='label')]\n",
    "    lr = [LogisticRegression(maxIter=100)] # lr: train a logistic regression model\n",
    "    pipeline = Pipeline(stages=tokenizer+ngrams+cv+idf+assembler+label+lr)\n",
    "    return pipeline\n",
    "\n",
    "# below function for training model or using model for classification from trained and saved model\n",
    "\n",
    "def main(sqlc,input_dir,loaded_model=None):\n",
    "    print('reading data from {}'.format(input_dir))\n",
    "    if not loaded_model:\n",
    "        train_set = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('train_data.csv')\n",
    "    test_set = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('test_data.csv')\n",
    "    print('preprocessing data...')\n",
    "    classify_pp = fun.udf(pre_processing, t.StringType())\n",
    "    if not loaded_model:\n",
    "        train_set = train_set.withColumn('tweet', classify_pp(f.col('text')))\n",
    "    test_set = test_set.withColumn('tweet', classify_pp(f.col('text')))\n",
    "    if not loaded_model:\n",
    "        pipeline = build_pipeline()\n",
    "        print('training...')\n",
    "        model = pipeline.fit(train_set)\n",
    "    else:\n",
    "        model = loaded_model\n",
    "    print('making predictions on test data...')\n",
    "    predictions = model.transform(test_set)\n",
    "    accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test_set.count())\n",
    "    return model, predictions, accuracy\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # create a SparkContext while checking if there is already SparkContext created\n",
    "    try:\n",
    "        sc = SparkContext(appName=\"YourTest\", master=\"local[2]\")\n",
    "        sc.setLogLevel(\"ERROR\")\n",
    "        sqlContext = SQLContext(sc)\n",
    "        print('Created a SparkContext')\n",
    "    except ValueError:\n",
    "        warnings.warn('SparkContext already exists in this scope')\n",
    "    # fitting the model after preprocessing\n",
    "    pipelineFit, predictions, accuracy = main(sqlContext,inputdir)\n",
    "    print('test data accuracy {}'.format(accuracy))\n",
    "    # saving model predictions\n",
    "    print('predictions saved to {}'.format(outputfile))\n",
    "    # saving the trained model to defined location\n",
    "    print('saving model to {}'.format(modeldir))\n",
    "    pipelineFit.save(modeldir)\n",
    "    # Testing model predictions\n",
    "    loadedModel = PipelineModel.load(modeldir)\n",
    "    _, _, loaded_accuracy = main(sqlContext,inputdir,loadedModel)\n",
    "    print('accuracy of the model is {}'.format(loaded_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelineModel' object has no attribute 'coefficientMatrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cb8422983f9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'com.databricks.spark.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Multinomial coefficients: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoefficientMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Multinomial intercepts: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterceptVector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelineModel' object has no attribute 'coefficientMatrix'"
     ]
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
