{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# specify the location of spark installation\n",
    "import findspark\n",
    "findspark.init(\"/home/aritra/spark/spark-2.4.0-bin-hadoop2.7\")\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "from collections import Counter\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import Row, SQLContext\n",
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import Tokenizer, NGram, CountVectorizer, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "\n",
    "#path where model is will be read by streaming application\n",
    "modeldir = \"/home/aritra/CS 631/Project/Birendra/model\"\n",
    "\n",
    "\n",
    "\n",
    "# Using regex for preprocessing\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1,pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "# preprocessing codes\n",
    "# remove Twitter handle and URL, remove URL pattern starting with www., and transform to lower characters and remove numbers and special characters\n",
    "\n",
    "def pre_processing(column):\n",
    "    step1 = re.sub(combined_pat, '', column)\n",
    "    step2 = re.sub(www_pat, '', step1)\n",
    "    step3 = step2.lower()\n",
    "    step4 = neg_pattern.sub(lambda x: negations_dic[x.group()], step3)\n",
    "    final = re.sub(r'[^A-Za-z ]','',step4)\n",
    "    return final.strip()\n",
    "\n",
    "# building a pipeline following below order\n",
    "# tokenizer + create n-gram + count vceorizer + inverse doc freq + assembler+  encoding target labels\n",
    "def build_pipeline():\n",
    "    tokenizer = [Tokenizer(inputCol='tweet',outputCol='words')]\n",
    "    ngrams = [NGram(n=i, inputCol='words', outputCol='{0}_grams'.format(i)) for i in range(1,4)]\n",
    "    cv = [CountVectorizer(vocabSize=5460, inputCol='{0}_grams'.format(i), outputCol='{0}_tf'.format(i)) for i in range(1,4)]\n",
    "    idf = [IDF(inputCol='{0}_tf'.format(i), outputCol='{0}_tfidf'.format(i), minDocFreq=5) for i in range(1,4)]\n",
    "    assembler = [VectorAssembler(inputCols=['{0}_tfidf'.format(i) for i in range(1,4)], outputCol='features')]\n",
    "    label = [StringIndexer(inputCol='sentiment', outputCol='label')]\n",
    "    lr = [LogisticRegression(maxIter=100)]\n",
    "    pipeline = Pipeline(stages=tokenizer+ngrams+cv+idf+assembler+label+lr)\n",
    "    return pipeline\n",
    "\n",
    "#function to compute the trending hash tag \n",
    "\n",
    "list2=[]\n",
    "def trending_hash_tag(row):\n",
    "    global list2\n",
    "    C = Counter(list2)\n",
    "    string = str(row[\"Text\"])\n",
    "    tag = re.findall(r\"#(\\w+)\", string)\n",
    "    dict1={}\n",
    "    if len(tag)>0:\n",
    "        for element in tag:\n",
    "            dict1[element]= C[element]\n",
    "        trending_hashtag= sorted(dict1)[0]\n",
    "    else:\n",
    "        trending_hashtag=\"\"\n",
    "    return trending_hashtag\n",
    "\n",
    "#function to compute the hash tag from tweet \n",
    "\n",
    "list1 =[]\n",
    "def hash_tag(row):\n",
    "    global list1\n",
    "    string = str(row[\"Text\"])\n",
    "    tag = re.findall(r\"#(\\w+)\", string)\n",
    "    list1.append(tag) \n",
    "    return \" \".join(tag)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create Spark instance with the above spark configuration\n",
    "sc = SparkContext(appName=\"YourTest\", master=\"local[2]\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc = StreamingContext(sc, 20) # 20 second batch interval\n",
    "sqlContext = SQLContext(sc)\n",
    "IP = \"localhost\" # Replace with your stream IP\n",
    "Port = 5000 # Replace with your stream port\n",
    "\n",
    "lines = ssc.socketTextStream(IP, Port)\n",
    "# split each tweet into words\n",
    "data_frame= pd.DataFrame(columns=['Time','longitude', 'latitude','place', \"Text\"])\n",
    "#function definition for save to csv file\n",
    "def save_df(rdd):\n",
    "    global data_frame\n",
    "    #print(rdd)\n",
    "    rdd1 = rdd.collect()\n",
    "    list1 = [x for x in rdd1 if len(x)>1]\n",
    "    list2= [line.split(\"*////*\") for line in list1 if len(line.split(\"*////*\"))==5]\n",
    "    #print(list2)\n",
    "    dummy_frame = pd.DataFrame(list2,columns = ['Time','longitude', 'latitude','place', \"Text\"] )\n",
    "    data_frame =data_frame.append(dummy_frame, ignore_index=True)\n",
    "    data_frame.to_csv(\"new_result.csv\")\n",
    "    \n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "\n",
    "\n",
    "    classify_pp = fun.udf(pre_processing)\n",
    "    model = PipelineModel.load(modeldir)\n",
    "    test_set = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('new_result.csv')\n",
    "    test_set = test_set.withColumn('tweet', classify_pp(fun.col('Text')))\n",
    "    predictions = model.transform(test_set)\n",
    "    predictions.createOrReplaceTempView(\"table1\")\n",
    "    df2 = predictions.select(predictions[\"time\"],predictions[\"longitude\"],predictions[\"latitude\"],predictions[\"place\"],predictions[\"Text\"], predictions[\"prediction\"])\n",
    "    df3 = df2.withColumn('Sentiment',when(df2.prediction == 0,\"Positive\").otherwise('Negative')).drop(df2.prediction)\n",
    "    #df3.show()\n",
    "    df4=df3.toPandas()\n",
    "    df4[\"hashstag\"] = df4.apply(hash_tag,axis =1)\n",
    "    df4[\"Trending hashtag\"]= df4.apply(trending_hash_tag,axis=1)\n",
    "    df4.to_csv(\"result_sample.csv\")\n",
    "\n",
    "\n",
    "lines.foreachRDD(save_df)\n",
    "\n",
    "ssc.start()\n",
    "# wait for the streaming to finish\n",
    "ssc.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
